###데이터
([샘플의 개수], 가로, 세로, [채널개수])

###딥러닝
 -이진 분류(예 : )
  맨 마지막층 뉴런의 구조 : 한개의 뉴런, 활성화 함수(sigmoid)
  loss : binary_crossentropy

 -다항 분류
  맨 마지막층 뉴런의 구조 : 범주의 개수, 활성화 함수(softmax)
  loss : categorical_crossentropy

 -연속형 변수 예측
  맨 마직막층 뉴런의 구조 : 뉴런의 개수 - 1개, 활성화 함수(X)


ann dnn cnn rnn

#완전 연결층(FCL - fully connected layer)
전체 뉴런이 연결이 되어 있다.

#합성곱 신경망
컨볼루션 계층
- 이미지가 있으면 필터가 존재하고, 
  필터를 스트라이드를 하면서 합성곱 연산을 통해서 값을 요약 및 압축
- 스트라이드의 개념 : 이동 간격
- 스트라이드를 1, 2로 했을 때, 어떤 결과의 차이
4x4 필터 3x3 스트라이드 1 : 2x2
4x4 필터 3x3 스트라이드 2 : 1x1
5x5 필터 3x3 스트라이드 1 : 3x3
5x5 필터 3x3 스트라이드 2 : 2x2

-필터와 이미지의 합성곱 연산을 통해서 생성되는 결과물
-특징맵(feature map) or activation map

-3x3 필터를 16개 씀, 특징맵의 개수 : 16개
-합성곱 연산은 필터의 개수만큼 특징맵을 만들어 낸다

-28, 28, 3 -> 3x3 필터의 개수 16
이 친구의 가중치의 개수 : 3x3x16x3(위의 3을 받는 거(28,28,3)에서의 3)+16x1

CNN의 장점
-01. 파라미터를 줄여준다.
-02. 공간 정보 손실을 방지해 준다.

-풀링 계층
 A. MaxPooling - 가장 큰 값을 가져온다.
 B. AveragePooling - 평균
 C. MinPooling - 낮은 값
   -가중치 없음.

-패딩
 A. zero padding이 이루어지면, 결과물은 사이즈가 그대로 유지.
 B. 특성맵 이미지가 줄어드는 것을 막기 위해 고안된 방법.

-배치 사이즈
 A. 1회 학습에 데이터를 쪼개서 돌린다. 그 사이즈를 가르킨다.